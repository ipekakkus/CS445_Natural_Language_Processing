{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TXP5nalr9ls"
      },
      "source": [
        "# **My Tokenizer**\n",
        "\n",
        "In this assignment, you are asked to create your own word tokenizer without the help of external tokenizers. Steps to the assignment:\n",
        "1. Choose one of the corpora from nltk.corpus list given - assign it to corpus_name\n",
        "1. Create your tokenizer in the code block - tokenize the selected corpus into token_list\n",
        "1. Give the raw corpus text, corpus_raw, and the my_token_list to the evaluation block\n",
        "\n",
        "Only splitting on whitespace is not enough. At least try two other improvements on the tokenization. Please write sufficient comments to show your reasoning.\n",
        "\n",
        "## Rules\n",
        "### Allowed:\n",
        " - Choosing a top-down tokenizer or bottom-up tokenizer\n",
        " - Using regular expressions library (import re)\n",
        " - Adding additional coding blocks\n",
        " - Having an additional dataset if you are creating a bottom-up tokenizer but you need to be able to run the code standalone.\n",
        "\n",
        "### Not allowed:\n",
        " - Using tokenizer libraries such as nltk.tokenize, or any other external libraries to tokenize.\n",
        " - Changing the contents of the evaluation block at the end of the notebook.\n",
        "\n",
        "## Assignment Report\n",
        "Please write a short assignment report at the end of the notebook (max 500 words). Please include all of the following points in the report:\n",
        " - Corpus name and the selection reason\n",
        " - Design of the tokenizer and reasoning\n",
        " - Challenges you have faced while writing the tokenizer and challenges with the specific corpus\n",
        " - Limitations of your approach\n",
        " - Possible improvements to the system\n",
        "\n",
        "## Grading\n",
        "You will be graded with the following criteria:\n",
        " - running complete code (0.5),\n",
        " - tokenizer algorithm (2),\n",
        " - clear commenting (0.5),\n",
        " - evaluation score - comparison with nltk word tokenizer (at most 1 point),\n",
        " - assignment report (1).\n",
        "\n",
        "## Submission\n",
        "\n",
        "Submission will be made to SUCourse. Please submit your file using the following naming convention.\n",
        "\n",
        "\n",
        "`studentid_studentname_tokenizer.ipynb Â - ex. 26744_aysegulrana_tokenizer.ipynb`\n",
        "\n",
        "\n",
        "**Deadline is October 22nd, 5pm.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_UZupSbvN-3",
        "outputId": "b6b98cd3-e238-4188-c27a-2aba200530ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f3j_JMbedNcG"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "13V4mcD8dFzy"
      },
      "outputs": [],
      "source": [
        "def extract_country_pair(input_string):\n",
        "    # Regex patterns for different combinations\n",
        "    pattern = r'([A-Z](?:\\.[A-Z])+|[A-Z][a-zA-Z]*)-([A-Z](?:\\.[A-Z])+|[A-Z][a-zA-Z]*)'\n",
        "\n",
        "    # Find all matches using the pattern\n",
        "    matches = re.findall(pattern, input_string)\n",
        "\n",
        "    # Combine matches to form full country pairs\n",
        "    result = [f\"{match[0]}-{match[1]}\" for match in matches]\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i_5ZyiIdJu5",
        "outputId": "ad22d3bc-8001-4ca4-e047-13c7f93787be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['U.S.A-Japan']\n",
            "['Turkey-U.K']\n",
            "['U.S.A-U.K']\n",
            "['Turkey-China']\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_string1 = \"U.S.A-Japan\"\n",
        "input_string2 = \"Turkey-U.K\"\n",
        "input_string3 = \"U.S.A-U.K\"\n",
        "input_string4 = \"Turkey-China\"\n",
        "\n",
        "# Extract and print results\n",
        "print(extract_country_pair(input_string1))  # Output: ['U.S.A-Japan']\n",
        "print(extract_country_pair(input_string2))  # Output: ['Turkey-U.K']\n",
        "print(extract_country_pair(input_string3))  # Output: ['U.S.A-U.K']\n",
        "print(extract_country_pair(input_string4))  # Output: ['Turkey-China']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MEx1a4vYd7_h"
      },
      "outputs": [],
      "source": [
        "def extract_hyphenated_words(input_string):\n",
        "    # Regex pattern for hyphenated compound words\n",
        "    pattern = r'\\b\\w+(?:-\\w+)+\\b'\n",
        "\n",
        "    # Find all matches using the pattern\n",
        "    matches = re.findall(pattern, input_string)\n",
        "    return matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfud9SyAd9YR",
        "outputId": "a9dae9ce-9330-4ade-c04d-5280e00eb252"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['well-being']\n",
            "['long-run']\n",
            "['up-to-date']\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_string1 = \"The well-being of the community is important.\"\n",
        "input_string2 = \"They worked together for a long-run success.\"\n",
        "input_string3 = \"The project is related to up-to-date technology.\"\n",
        "\n",
        "# Extract and print results\n",
        "print(extract_hyphenated_words(input_string1))  # Output: ['well-being']\n",
        "print(extract_hyphenated_words(input_string2))  # Output: ['long-run']\n",
        "print(extract_hyphenated_words(input_string3))  # Output: ['up-to-date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "waiYobAHeyXf"
      },
      "outputs": [],
      "source": [
        "def handle_contractions(tokens):\n",
        "    # List to hold the processed tokens\n",
        "    processed_tokens = []\n",
        "\n",
        "    # Regex to match common contractions\n",
        "    contraction_pattern = r\"(\\w+)(n't|'ll|'ve|'re|'d|'m)\"\n",
        "\n",
        "    # Iterate through each token to split contractions\n",
        "    for token in tokens:\n",
        "        match = re.match(contraction_pattern, token)\n",
        "        if match:\n",
        "            processed_tokens.extend([match.group(1), match.group(2)])\n",
        "        else:\n",
        "            processed_tokens.append(token)\n",
        "\n",
        "    return processed_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUPSED5TvWsI",
        "outputId": "db225ed3-e554-4957-80fe-0b72e54310cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ca', \"n't\", 'he', \"'ll\", 'I', \"'ve\", 'you', \"'re\", 'they', \"'d\", 'I', \"'m\", 'running']\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "tokens = [\"can't\", \"he'll\", \"I've\", \"you're\", \"they'd\", \"I'm\", \"running\"]\n",
        "\n",
        "# Handle contractions in the list of tokens\n",
        "print(handle_contractions(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "COSDeenZfTzh"
      },
      "outputs": [],
      "source": [
        "def handle_possessives(tokens):\n",
        "    # List to hold the processed tokens\n",
        "    processed_tokens = []\n",
        "\n",
        "    # Regex to match possessives ending in 's (e.g., \"China's\")\n",
        "    possessive_pattern = r\"(\\w+)('s)\"\n",
        "\n",
        "    # Iterate through each token to split possessives\n",
        "    for token in tokens:\n",
        "        match = re.match(possessive_pattern, token)\n",
        "        if match:\n",
        "            processed_tokens.extend([match.group(1), match.group(2)])\n",
        "        else:\n",
        "            processed_tokens.append(token)\n",
        "\n",
        "    return processed_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfg-s0BYoOlH",
        "outputId": "a266fe0d-49cc-4e65-a2e3-0b1845d4dae1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['China', \"'s\", 'children', \"'s\", 'Japan', 'it', \"'s\", 'Tom', \"'s\", 'running']\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "tokens = [\"China's\", \"children's\", \"Japan\", \"it's\", \"Tom's\", \"running\"]\n",
        "\n",
        "# Handle possessives in the list of tokens\n",
        "print(handle_possessives(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pyBgEmzVfU49"
      },
      "outputs": [],
      "source": [
        "def handle_decimals(text):\n",
        "    # Regex pattern to match decimal numbers (e.g., 15.6, 0.001)\n",
        "    pattern = r'\\b\\d+\\.\\d+\\b'\n",
        "\n",
        "    # Find all matches using the pattern\n",
        "    tokens = re.findall(pattern, text)\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mflJFOBnj2D",
        "outputId": "8e50d132-0606-4af3-fe95-520a744481bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['15.6', '0.05']\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "sample_text = \"\"\"The GDP was 15.6 billion dlrs last year, among the world's largest. The profit margin was 0.05%.\"\"\"\n",
        "\n",
        "print(handle_decimals(sample_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TWKW1i3mvZ7A"
      },
      "outputs": [],
      "source": [
        "def handle_eras(text):\n",
        "    # Regex pattern to match eras (e.g., mid-1988, early-2000s, late-19th)\n",
        "    pattern = r'\\b(?:mid|early|late)-\\d{4}(?:s)?\\b|\\b(?:mid|early|late)-\\d{1,2}(?:st|nd|rd|th)?\\b'\n",
        "\n",
        "    # Find all matches using the pattern\n",
        "    tokens = re.findall(pattern, text)\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JW0ae1Eovhj_",
        "outputId": "1a5870e4-4031-4c56-d06a-8837d2576e83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['mid-1988', 'late-20th']\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "sample_text = \"\"\"The GDP was 15.6 billion dlrs mid-1988. On the other hand, late-20th century has not began yet\"\"\"\n",
        "\n",
        "print(handle_eras(sample_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "j8TKxzowvl0X"
      },
      "outputs": [],
      "source": [
        "def handle_parentheses(text):\n",
        "    # Regex pattern to specifically match parentheses (), [], {}\n",
        "    pattern = r'[\\(\\)\\[\\]\\{\\}]'\n",
        "\n",
        "    # Find all matches using the pattern\n",
        "    tokens = re.findall(pattern, text)\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbz9RHYYv8Wy",
        "outputId": "b1b49250-d29f-49a5-f24b-446cedb28e80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['(', '{', '}', ']']\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "sample_text = \"\"\"erosion of\n",
        "  exports (of goods subject to {}tariffs] to the\"\"\"\n",
        "\n",
        "print(handle_parentheses(sample_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Xr_BBt-cv-d8"
      },
      "outputs": [],
      "source": [
        "def handle_special_symbols(text):\n",
        "    # Regex pattern to specifically match $, %, #, <, >, ?, _, /, \\, +, -, *, /\n",
        "    pattern = r'[\\$%#&@<>\\!\\\\\"\\?_\\/\\+\\-\\*]'\n",
        "\n",
        "    # Find all matches using the pattern\n",
        "    tokens = re.findall(pattern, text)\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv0xxGFBxtQo",
        "outputId": "e5ce6745-ad65-4095-eb7c-fc70c00e5e7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['&', '$', '/', '\"', '@', '_', '>']\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "sample_text = \"\"\"&lt;Taiwan$/; \"If Safe@ _Group>\"\"\"\n",
        "\n",
        "print(handle_special_symbols(sample_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2BOEnvT1EbH1"
      },
      "outputs": [],
      "source": [
        "def handle_date_formats(text):\n",
        "    # Regex pattern to match dates in various formats (e.g., 01/12/2023, 1.12.2023)\n",
        "    pattern = r'\\b(0?[1-9]|[12][0-9]|3[01])[/-](0?[1-9]|1[0-2])[/-](\\d{4})\\b|\\b(0?[1-9]|[12][0-9]|3[01])\\.(0?[1-9]|1[0-2])\\.(\\d{4})\\b'\n",
        "\n",
        "    # Find all matches using the pattern\n",
        "    matches = re.finditer(pattern, text)\n",
        "\n",
        "    tokens = []\n",
        "    for match in matches:\n",
        "        tokens.append(match.group())\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13fQRzjMEb6B",
        "outputId": "cdb46c8f-42cd-4bd7-d5fd-927535991af6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['20/10/2024', '21.10.2024']\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "sample_text = \"\"\"I saw him at 20/10/2024 and 21.10.2024\"\"\"\n",
        "\n",
        "print(handle_date_formats(sample_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Y1aQp0V6Srcd"
      },
      "outputs": [],
      "source": [
        "def handle_commas_dots_colons_semicolons(text):\n",
        "    # Regex pattern to match commas, dots, colons, and semicolons as standalone tokens\n",
        "    pattern = r'[,:;\\.]'\n",
        "\n",
        "    # Find all matches using the pattern\n",
        "    tokens = re.findall(pattern, text)\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0092r-ldTD4n",
        "outputId": "1e6fe94f-6340-42ab-ff9d-39081965dcff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[';', ';', '.', ',', ':']\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "sample_text = \"\"\"&lt;Taiwan$/; \"If., S:afe@ _Group>\"\"\"\n",
        "\n",
        "print(handle_commas_dots_colons_semicolons(sample_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "-jYyH_qz3Lii"
      },
      "outputs": [],
      "source": [
        "def my_tokenizer(text):\n",
        "    '''\n",
        "    type corpus_raw: string\n",
        "    param corpus_raw: The raw output of the corpus to be tokenized\n",
        "    rtype: list\n",
        "    return: a list of tokens extracted from the corpus_raw\n",
        "    '''\n",
        "    # Split text by whitespace to get initial tokens\n",
        "    tokens = text.split()\n",
        "    token_list = []\n",
        "\n",
        "    # Process each token using different handlers\n",
        "    while tokens:\n",
        "        token = tokens.pop(0)\n",
        "\n",
        "        # Step 1: Handle decimal numbers\n",
        "        decimals_tokens= handle_decimals(token)\n",
        "        if decimals_tokens:\n",
        "            token_list.extend(decimals_tokens)\n",
        "            continue\n",
        "\n",
        "        # Step 2: Extract country or abbreviation pairs\n",
        "        abbreviation_pairs_tokens= extract_country_pair(token)\n",
        "        if abbreviation_pairs_tokens:\n",
        "            token_list.extend(abbreviation_pairs_tokens)\n",
        "            continue\n",
        "\n",
        "        # Step 3: Extract hyphenated words\n",
        "        hyphenated_words_tokens = extract_hyphenated_words(token)\n",
        "        if hyphenated_words_tokens:\n",
        "            token_list.extend(hyphenated_words_tokens)\n",
        "            continue\n",
        "\n",
        "        # Step 4: Extract eras\n",
        "        era_tokens = handle_eras(token)\n",
        "        if era_tokens:\n",
        "            token_list.extend(era_tokens)\n",
        "            continue\n",
        "\n",
        "        # Step 5: Extract dates\n",
        "        date_tokens = handle_date_formats(token)\n",
        "        if date_tokens:\n",
        "            token_list.extend(date_tokens)\n",
        "            continue\n",
        "\n",
        "        # Step 6: Extract parentheses\n",
        "        parantheses_tokens = handle_parentheses(token)\n",
        "        if parantheses_tokens:\n",
        "            token_list.extend(parantheses_tokens)\n",
        "            continue\n",
        "\n",
        "        # Step 9: Extract special symbols ($, %, #, <, >, ?, _, /, \\, +, -, *, /, \")\n",
        "        if re.match(r'[\\$%#&@<>\\\"\\?_\\/\\+\\-\\*]', token):\n",
        "            token_list.extend(handle_special_symbols(token))\n",
        "            continue\n",
        "\n",
        "        # Step 10: Extract commas, dots, colons, and semicolons\n",
        "        if re.match(r'[,:;\\.]', token):\n",
        "            token_list.extend(handle_commas_dots_colons_semicolons(token))\n",
        "            continue\n",
        "\n",
        "        # General case: Add the token to the list\n",
        "        token_list.append(token)\n",
        "\n",
        "    # Step 11: Handle possessives in the tokens\n",
        "    token_list = handle_possessives(token_list)\n",
        "\n",
        "    # Step 12: Handle contractions in the tokens\n",
        "    token_list = handle_contractions(token_list)\n",
        "\n",
        "    # Step 13: Convert all tokens to lowercase\n",
        "    token_list = [token.lower() for token in token_list]\n",
        "\n",
        "    return token_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkoFSRdVTrKT",
        "outputId": "705327ff-7440-457a-dbec-6c7a468d8414"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['korea', \"'s\", 'economic', 'growth,', 'mid-1988', 'late-19th', 'century,', '15.6', 'billion', 'dlrs', 'last', 'year,', 'among', 'the', 'world', \"'s\", 'largest.', 'the', 'profit', 'margin', 'was', '0.05', 'symbols', 'like', '<', '>', '(', ')', '[', ']', '{', '}', '\"', '\"', '&', '>', 'and', '&', 'others', 'should', 'be', 'handled', 'correctly.', 'the', 'price', 'is', '$', 'and', 'the', 'rate', 'is', '5%.', 'use', '#', 'for', 'posts.', 'special', 'symbols', 'like', '_', '/', '\\\\', '+', '-', '*', 'are', 'also', 'included.', 'the', 'event', 'was', 'held', 'on', '01/12/2023', 'and', '1.12']\n"
          ]
        }
      ],
      "source": [
        "# Sample text to test the tokenizer\n",
        "sample_text = \"\"\"Korea's economic growth, mid-1988, late-19th century, 15.6 billion dlrs last year, among the world's largest. The profit margin was 0.05%. Symbols like <>, (), [], {}, \"\", &lt;MC.T> and & others should be handled correctly. The price is $100 and the rate is 5%. Use #hashtag for posts. Special symbols like _ / \\ + - * are also included. The event was held on 01/12/2023 and 1.12.2023.\"\"\"\n",
        "\n",
        "# Tokenize the sample text\n",
        "token_list = my_tokenizer(sample_text)\n",
        "print(token_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HP0awlu3jci"
      },
      "source": [
        "You are allowed to add code blocks above to use for your tokenizer or evaluate it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZF4WJjKxZfc",
        "outputId": "abf3a99e-e376-40a0-b089-8422f60024d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\n",
            "  Mounting trade friction between the\n",
            "  U.S. And Japan has raised fears among many of Asia's exporting\n",
            "  nations that the row could inflict far-reaching economic\n",
            "  damage, businessmen and officials said.\n",
            "      They told Reuter correspondents in Asian capitals a U.S.\n",
            "  Move against Japan might boost protectionist sentiment in the\n",
            "  U.S. And lead to curbs on American imports of their products.\n",
            "      But some exporters said that while the conflict would hurt\n",
            "  them in the long-run, in the short-term Tokyo's loss might be\n",
            "  their gain.\n",
            "      The U.S. Has said it will impose 300 mln dlrs of tariffs on\n",
            "  imports of Japanese electronics goods on Apri\n",
            "['asian', 'exporters', 'fear', 'damage', 'from', 'u.s.-japan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'u.s.', 'and', 'japan', 'has', 'raised', 'fears', 'among', 'many']\n",
            "['of', 'asia', \"'s\", 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far-reaching', 'economic', 'damage,', 'businessmen', 'and', 'officials', 'said.', 'they', 'told', 'reuter']\n",
            "['correspondents', 'in', 'asian', 'capitals', 'a', 'u.s.', 'move', 'against', 'japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'u.s.', 'and', 'lead', 'to', 'curbs']\n",
            "['on', 'american', 'imports', 'of', 'their', 'products.', 'but', 'some', 'exporters', 'said', 'that', 'while', 'the', 'conflict', 'would', 'hurt', 'them', 'in', 'the', 'long-run']\n",
            "['in', 'the', 'short-term', 'tokyo', \"'s\", 'loss', 'might', 'be', 'their', 'gain.', 'the', 'u.s.', 'has', 'said', 'it', 'will', 'impose', '300', 'mln', 'dlrs']\n",
            "['of', 'tariffs', 'on', 'imports', 'of', 'japanese', 'electronics', 'goods', 'on', 'april', '17,', 'in', 'retaliation', 'for', 'japan', \"'s\", 'alleged', 'failure', 'to', 'stick']\n",
            "['to', 'a', 'pact', 'not', 'to', 'sell', 'semiconductors', 'on', 'world', 'markets', 'at', 'below', 'cost.', 'unofficial', 'japanese', 'estimates', 'put', 'the', 'impact', 'of']\n",
            "['the', 'tariffs', 'at', '10', 'billion', 'dlrs', 'and', 'spokesmen', 'for', 'major', 'electronics', 'firms', 'said', 'they', 'would', 'virtually', 'halt', 'exports', 'of', 'products']\n",
            "['hit', 'by', 'the', 'new', 'taxes.', '\"', 'would', \"n't\", 'be', 'able', 'to', 'do', 'business,\"', 'said', 'a', 'spokesman', 'for', 'leading', 'japanese', 'electronics']\n",
            "['firm', 'matsushita', 'electric', 'industrial', 'co', 'ltd', '&', '>', '\"', 'the', 'tariffs', 'remain', 'in', 'place', 'for', 'any', 'length', 'of', 'time', 'beyond']\n",
            "['a', 'few', 'months', 'it', 'will', 'mean', 'the', 'complete', 'erosion', 'of', 'exports', '(', 'goods', 'subject', 'to', ')', 'to', 'the', 'u.s.,\"', 'said']\n",
            "['tom', 'murtha,', 'a', 'stock', 'analyst', 'at', 'the', 'tokyo', 'office', 'of', 'broker', '&', 'capel', 'and', 'co>.', 'in', 'taiwan,', 'businessmen', 'and', 'officials']\n",
            "['are', 'also', 'worried.', '\"', 'are', 'aware', 'of', 'the', 'seriousness', 'of', 'the', 'u.s.', 'threat', 'against', 'japan', 'because', 'it', 'serves', 'as', 'a']\n",
            "['warning', 'to', 'us,\"', 'said', 'a', 'senior', 'taiwanese', 'trade', 'official', 'who', 'asked', 'not', 'to', 'be', 'named.', 'taiwan', 'had', 'a', 'trade', 'trade']\n",
            "['surplus', 'of', '15.6', 'billion', 'dlrs', 'last', 'year,', '95', 'pct', 'of', 'it', 'with', 'the', 'u.s.', 'the', 'surplus', 'helped', 'swell', 'taiwan', \"'s\"]\n",
            "['foreign', 'exchange', 'reserves', 'to', '53', 'billion', 'dlrs,', 'among', 'the', 'world', \"'s\", 'largest.', '\"', 'must', 'quickly', 'open', 'our', 'markets,', 'remove', 'trade']\n",
            "['barriers', 'and', 'cut', 'import', 'tariffs', 'to', 'allow', 'imports', 'of', 'u.s.', 'products,', 'if', 'we', 'want', 'to', 'defuse', 'problems', 'from', 'possible', 'u.s.']\n",
            "['retaliation,\"', 'said', 'paul', 'sheen,', 'chairman', 'of', 'textile', 'exporters', '&', 'safe', 'group>.', 'a', 'senior', 'official', 'of', 'south', 'korea', \"'s\", 'trade', 'promotion']\n",
            "['association', 'said', 'the', 'trade', 'dispute', 'between', 'the', 'u.s.', 'and', 'japan', 'might', 'also', 'lead', 'to', 'pressure', 'on', 'south', 'korea,', 'whose', 'chief']\n",
            "['exports', 'are', 'similar', 'to', 'those', 'of', 'japan.', 'last', 'year', 'south', 'korea', 'had', 'a', 'trade', 'surplus', 'of', '7.1', 'billion', 'dlrs', 'with']\n",
            "['the', 'u.s.,', 'up', 'from', '4.9', 'billion', 'dlrs', 'in', '1985.', 'in', 'malaysia,', 'trade', 'officers', 'and', 'businessmen', 'said', 'tough', 'curbs', 'against', 'japan']\n",
            "['might', 'allow', 'hard-hit', 'producers', 'of', 'semiconductors', 'in', 'third', 'countries', 'to', 'expand', 'their', 'sales', 'to', 'the', 'u.s.', 'in', 'hong', 'kong,', 'where']\n",
            "['newspapers', 'have', 'alleged', 'japan', 'has', 'been', 'selling', 'below-cost', 'semiconductors,', 'some', 'electronics', 'manufacturers', 'share', 'that', 'view.', 'but', 'other', 'businessmen', 'said', 'such']\n",
            "['a', 'short-term', 'commercial', 'advantage', 'would', 'be', 'outweighed', 'by', 'further', 'u.s.', 'pressure', 'to', 'block', 'imports.', '\"', 'is', 'a', 'very', 'short-term', 'view,\"']\n",
            "['said', 'lawrence', 'mills,', 'director-general', 'of', 'the', 'federation', 'of', 'hong', 'kong', 'industry.', '\"', 'the', 'whole', 'purpose', 'is', 'to', 'prevent', 'imports,', 'one']\n",
            "['day', 'it', 'will', 'be', 'extended', 'to', 'other', 'sources.', 'much', 'more', 'serious', 'for', 'hong', 'kong', 'is', 'the', 'disadvantage', 'of', 'action', 'restraining']\n",
            "['trade,\"', 'he', 'said.', 'the', 'u.s.', 'last', 'year', 'was', 'hong', 'kong', \"'s\", 'biggest', 'export', 'market,', 'accounting', 'for', 'over', '30', 'pct', 'of']\n",
            "['domestically', 'produced', 'exports.', 'the', 'australian', 'government', 'is', 'awaiting', 'the', 'outcome', 'of', 'trade', 'talks', 'between', 'the', 'u.s.', 'and', 'japan', 'with', 'interest']\n",
            "['and', 'concern,', 'industry', 'minister', 'john', 'button', 'said', 'in', 'canberra', 'last', 'friday.', '\"', 'kind', 'of', 'deterioration', 'in', 'trade', 'relations', 'between', 'two']\n",
            "['countries', 'which', 'are', 'major', 'trading', 'partners', 'of', 'ours', 'is', 'a', 'very', 'serious', 'matter,\"', 'button', 'said.', 'he', 'said', 'australia', \"'s\", 'concerns']\n",
            "['centred', 'on', 'coal', 'and', 'beef,', 'australia', \"'s\", 'two', 'largest', 'exports', 'to', 'japan', 'and', 'also', 'significant', 'u.s.', 'exports', 'to', 'that', 'country.']\n",
            "['meanwhile', 'u.s.-japanese', 'diplomatic', 'manoeuvres', 'to', 'solve', 'the', 'trade', 'stand-off', 'continue.', 'japan', \"'s\", 'ruling', 'liberal', 'democratic', 'party', 'yesterday', 'outlined', 'a', 'package']\n",
            "['of', 'economic', 'measures', 'to', 'boost', 'the', 'japanese', 'economy.', 'the', 'measures', 'proposed', 'include', 'a', 'large', 'supplementary', 'budget', 'and', 'record', 'public', 'works']\n",
            "['spending', 'in', 'the', 'first', 'half', 'of', 'the', 'financial', 'year.', 'they', 'also', 'call', 'for', 'stepped-up', 'spending', 'as', 'an', 'emergency', 'measure', 'to']\n",
            "['stimulate', 'the', 'economy', 'despite', 'prime', 'minister', 'yasuhiro', 'nakasone', \"'s\", 'avowed', 'fiscal', 'reform', 'program.', 'deputy', 'u.s.', 'trade', 'representative', 'michael', 'smith', 'and']\n",
            "['makoto', 'kuroda,', 'japan', \"'s\", 'deputy', 'minister', 'of', 'international', 'trade', 'and', 'industry', '(', ')', 'are', 'due', 'to', 'meet', 'in', 'washington', 'this']\n",
            "['week', 'in', 'an', 'effort', 'to', 'end', 'the', 'dispute.', 'china', 'daily', 'says', 'vermin', 'eat', '7-12', 'pct', 'grain', 'stocks', 'a', 'survey', 'of']\n",
            "['19', 'provinces', 'and', 'seven', 'cities', 'showed', 'vermin', 'consume', 'between', 'seven', 'and', '12', 'pct', 'of', 'china', \"'s\", 'grain', 'stocks,', 'the', 'china']\n"
          ]
        }
      ],
      "source": [
        "#main code to run your tokenizer.\n",
        "#import your libraries here\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "#select the corpus name from the list below\n",
        "#gutenberg, webtext, reuters, product_reviews_2\n",
        "corpus_name = 'reuters'\n",
        "\n",
        "#download the corpus and import it.\n",
        "nltk.download(corpus_name)\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "#get the raw text output of the corpus to the corpus_raw variable.\n",
        "file_ids = reuters.fileids()\n",
        "corpus_raw = ' '.join([reuters.raw(file_id) for file_id in file_ids])\n",
        "\n",
        "# Print a portion of the raw corpus to inspect\n",
        "print(corpus_raw[:700])\n",
        "\n",
        "#call your tokenizer method\n",
        "my_tokenized_list = my_tokenizer(corpus_raw)\n",
        "\n",
        "# Print the tokens in groups of 20\n",
        "for i in range(0, 750, 20): #len(my_tokenized_list)\n",
        "    print(my_tokenized_list[i:i+20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuTDStwY36cT"
      },
      "source": [
        "## Please do not touch the code below that will evaluate your tokenizer with the nltk word tokenizer. You will get zero points from evaluation if you do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "8txzw8Ag8ysD"
      },
      "outputs": [],
      "source": [
        "def similarity_score(set_a, set_b):\n",
        "    '''\n",
        "    type set_a: set\n",
        "    param set_a: The first set to be compared\n",
        "    type set_b: set\n",
        "    param set_b: The tokens extracted from the corpus_raw\n",
        "    rtype: float\n",
        "    return: similarity score with two sets using Jaccard similarity.\n",
        "    '''\n",
        "\n",
        "    jaccard_similarity = float(len(set_a.intersection(set_b)) / len(set_a.union(set_b)))\n",
        "\n",
        "    return jaccard_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wUTqReb36Hg",
        "outputId": "9c654714-fc70-495f-810e-28fd4f155d1d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk import punkt\n",
        "\n",
        "def evaluation(corpus_raw, token_list):\n",
        "    '''\n",
        "    type corpus_raw: string\n",
        "    param corpus_raw: The raw output of the corpus\n",
        "    type token_list: list\n",
        "    param token_list: The tokens extracted from the corpus_raw\n",
        "    rtype: float\n",
        "    return: comparison score with the given token list and the nltk tokenizer.\n",
        "    '''\n",
        "\n",
        "    #The comparison score only looks at the tokens but not the frequencies of the tokens.\n",
        "    #we assume case folding is already applied to the token_list\n",
        "    corpus_raw = corpus_raw.lower()\n",
        "    nltk_tokens = word_tokenize(corpus_raw, language='english')\n",
        "\n",
        "    score = similarity_score(set(token_list), set(nltk_tokens))\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vt_uSBF-NHm",
        "outputId": "d427bd58-0c8f-44c7-e2a9-000a5d2f375e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The similarity score is 0.65\n"
          ]
        }
      ],
      "source": [
        "#Evaluation\n",
        "\n",
        "eval_score = evaluation(corpus_raw, my_tokenized_list)\n",
        "\n",
        "print('The similarity score is {:.2f}'.format(eval_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keZ3UBqh4hgP"
      },
      "source": [
        "Please write your report below using clear headlines with markdown syntax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6SYejO_ctwF"
      },
      "source": [
        "# Assignment Report\n",
        "\n",
        "### Corpus name and the selection reason\n",
        "\n",
        "The chosen corpus for this assignment is the Reuters. \n",
        "It was selected as it contains diverse financial news articles that include abbreviations, punctuation, and complex structures. \n",
        "This diversity made it a suitable choice for testing the robustness and accuracy of the tokenizer. \n",
        "Additionally, since the dataset is expected to have minimal typos, it facilitates deeper analysis.\n",
        "\n",
        "### Design of the tokenizer and reasoning\n",
        "\n",
        "The tokenizer was designed using a modular approach, with each function handling a specific tokenization challenge. \n",
        "Functions were created to address decimals, possessives, contractions, hyphenated words, special symbols, and country pairs. \n",
        "Regular expressions were used for their flexibility in matching complex patterns. \n",
        "The modular design allowed for systematic debugging and easy extension to handle new cases.\n",
        "13 different cases are considered as well as splitting from whitespaces. These all are decided after careful observation of what NLTK does to tokenize the Reuters corpora in another script.\n",
        "After these observation, abbreviations(e.g. U.S.A), abbreviation pairs(e.g U.S.A-Japan), hypetenated words(e.g. far-reaching), eras (e.g. mid-1900), dates (e.g 23.10.2024 or 23/10/2024), parantheses (e.g ({[]})), possessives(e.g. Korea's), contractions(e.g wouldn't, he'll), special symbols and punctuations are identified as challenges to be tokenized seperately. All functions for these are tested with short sample strings.\n",
        "Unlike the NLTK tokenizer, all tokens are lowercased, in order to increase the similarity score at the end.\n",
        "\n",
        "### Challenges faced\n",
        "\n",
        "Challenges included handling edge cases without affecting other parts of the text. For example, decimals like \"-15.6\" required careful handling to ensure correct tokenization. Special characters like parentheses, brackets, and ampersands also posed difficulties in ensuring they were split correctly without losing string context. Additionally, the sequential processing of tokens sometimes led to incorrect splits due to overlapping patterns. Punctuations were mostly problematic, as they are both using as punctuations and for abbreviations or any other usages.\n",
        "\n",
        "### Limitations of the approach\n",
        "\n",
        "The tokenizer's sequential nature means that earlier transformations can affect subsequent ones, leading to inconsistencies. Functions can be enhanced to be nested more. For example the hypen's function can cover \"U.S.A-Japan\" type of abbreviations or inner hypetenated words such as \"well-being\", as well. The reliance on regular expressions also limits flexibility in handling ambiguous text. The tokenizer is rule-based and lacks the ability to learn from data, limiting its adaptability to new or unforeseen text structures.\n",
        "\n",
        "### Possible improvements\n",
        "\n",
        "To make the tokenizer closer to the NLTK tokenizer, more careful implementation of punctuation handling could be done. For example, handling cases like \"5%.\", \"posts.\", and \"<MC.T>\" with more specific punctuation rules would improve the accuracy. For \"5%\" case handle_decimals function should be enhanced to be able to tokenize any string before and after the number with decimals. Also, instead of checking some of the functions with regexs in my_Tokenizer function, the helper functions can be called directly, as the previous ones. Additionally, comprehensive testing with diverse corpora could help identify and address additional edge cases.\n",
        "\n",
        "---\n",
        "\n",
        "Ipek Akkus - 30800 - ipek.akkus"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
